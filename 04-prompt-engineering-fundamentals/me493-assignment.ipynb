{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottpcs/generative-ai-for-beginners-Scott/blob/main/04-prompt-engineering-fundamentals/me493-assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5spjWOHnTiWS"
      },
      "source": [
        "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-HBAV3TiWU"
      },
      "source": [
        "# Introduction to Prompt Engineering\n",
        "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io-BnzazTiWV"
      },
      "source": [
        "### Exercise 1: Tokenization\n",
        "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
        "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tiktoken\n",
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "id": "EdK5UT48vxTd",
        "outputId": "490a0418-3f92-4929-c3a0-d41b0c9091a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m225.3/262.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0Z3pbyqTiWV",
        "outputId": "28ded60f-bddd-4851-e4cc-3a8543070479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'\\n',\n",
              " b'J',\n",
              " b'upiter',\n",
              " b' is',\n",
              " b' the',\n",
              " b' fifth',\n",
              " b' planet',\n",
              " b' from',\n",
              " b' the',\n",
              " b' Sun',\n",
              " b' and',\n",
              " b' the',\n",
              " b' largest',\n",
              " b' in',\n",
              " b' the',\n",
              " b' Solar',\n",
              " b' System',\n",
              " b'.',\n",
              " b' It',\n",
              " b' is',\n",
              " b' a',\n",
              " b' gas',\n",
              " b' giant',\n",
              " b' with',\n",
              " b' a',\n",
              " b' mass',\n",
              " b' one',\n",
              " b'-th',\n",
              " b'ousand',\n",
              " b'th',\n",
              " b' that',\n",
              " b' of',\n",
              " b' the',\n",
              " b' Sun',\n",
              " b',',\n",
              " b' but',\n",
              " b' two',\n",
              " b'-and',\n",
              " b'-a',\n",
              " b'-half',\n",
              " b' times',\n",
              " b' that',\n",
              " b' of',\n",
              " b' all',\n",
              " b' the',\n",
              " b' other',\n",
              " b' planets',\n",
              " b' in',\n",
              " b' the',\n",
              " b' Solar',\n",
              " b' System',\n",
              " b' combined',\n",
              " b'.',\n",
              " b' Jupiter',\n",
              " b' is',\n",
              " b' one',\n",
              " b' of',\n",
              " b' the',\n",
              " b' brightest',\n",
              " b' objects',\n",
              " b' visible',\n",
              " b' to',\n",
              " b' the',\n",
              " b' naked',\n",
              " b' eye',\n",
              " b' in',\n",
              " b' the',\n",
              " b' night',\n",
              " b' sky',\n",
              " b',',\n",
              " b' and',\n",
              " b' has',\n",
              " b' been',\n",
              " b' known',\n",
              " b' to',\n",
              " b' ancient',\n",
              " b' civilizations',\n",
              " b' since',\n",
              " b' before',\n",
              " b' recorded',\n",
              " b' history',\n",
              " b'.',\n",
              " b' It',\n",
              " b' is',\n",
              " b' named',\n",
              " b' after',\n",
              " b' the',\n",
              " b' Roman',\n",
              " b' god',\n",
              " b' Jupiter',\n",
              " b'.[',\n",
              " b'19',\n",
              " b']',\n",
              " b' When',\n",
              " b' viewed',\n",
              " b' from',\n",
              " b' Earth',\n",
              " b',',\n",
              " b' Jupiter',\n",
              " b' can',\n",
              " b' be',\n",
              " b' bright',\n",
              " b' enough',\n",
              " b' for',\n",
              " b' its',\n",
              " b' reflected',\n",
              " b' light',\n",
              " b' to',\n",
              " b' cast',\n",
              " b' visible',\n",
              " b' shadows',\n",
              " b',[',\n",
              " b'20',\n",
              " b']',\n",
              " b' and',\n",
              " b' is',\n",
              " b' on',\n",
              " b' average',\n",
              " b' the',\n",
              " b' third',\n",
              " b'-b',\n",
              " b'right',\n",
              " b'est',\n",
              " b' natural',\n",
              " b' object',\n",
              " b' in',\n",
              " b' the',\n",
              " b' night',\n",
              " b' sky',\n",
              " b' after',\n",
              " b' the',\n",
              " b' Moon',\n",
              " b' and',\n",
              " b' Venus',\n",
              " b'.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# EXERCISE:\n",
        "# 1. Run the exercise as is first\n",
        "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Define the prompt you want tokenized\n",
        "text = f\"\"\"\n",
        "Jupiter is the fifth planet from the Sun and the \\\n",
        "largest in the Solar System. It is a gas giant with \\\n",
        "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
        "times that of all the other planets in the Solar System combined. \\\n",
        "Jupiter is one of the brightest objects visible to the naked eye \\\n",
        "in the night sky, and has been known to ancient civilizations since \\\n",
        "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
        "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
        "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
        "natural object in the night sky after the Moon and Venus.\n",
        "\"\"\"\n",
        "\n",
        "# Set the model you want encoding for\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Encode the text - gives you the tokens in integer form\n",
        "tokens = encoding.encode(text)\n",
        "print(tokens);\n",
        "\n",
        "# Decode the integers to see what the text versions look like\n",
        "[encoding.decode_single_token_bytes(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWHAA3mSTiWW"
      },
      "source": [
        "### Exercise 2: Validate OpenAI API Key Setup\n",
        "\n",
        "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI package (Uncomment the line below if needed)\n",
        "# !pip install openai --upgrade\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "from google.colab import userdata\n",
        "OPENAI_KEY = userdata.get('ME493')\n",
        "#OPENAI_KEY = 'OPENAI_API_KEY'\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "def get_completion(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    )\n",
        "    # Extracting message content from the returned response object\n",
        "    # Adjusted to extract the 'content' correctly based on the actual structure of the response\n",
        "    first_choice_message = response.choices[0].message\n",
        "    if 'content' in first_choice_message:\n",
        "        return first_choice_message['content']\n",
        "    else:\n",
        "        # Handle cases where 'content' may not be a direct attribute of the response\n",
        "        return str(first_choice_message)\n",
        "\n",
        "# Call the helper method\n",
        "\n",
        "# Primary content or prompt text\n",
        "text = \"oh say can you see\"\n",
        "\n",
        "# Using the prompt\n",
        "response = get_completion(text)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "TdmEiA0wwjxV",
        "outputId": "6ffc1b84-846f-43ff-fc81-ba4c85ead65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content=\"By the dawn's early light\", role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenAI package (Uncomment the line below if needed)\n",
        "# !pip install openai --upgrade\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "from google.colab import userdata\n",
        "OPENAI_KEY = userdata.get('ME493')\n",
        "#OPENAI_KEY = 'OPENAI_API_KEY'\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "def get_completion(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    )\n",
        "    # Extracting message content from the returned response object\n",
        "    # Adjusted to extract the 'content' correctly based on the actual structure of the response\n",
        "    return response.choices[0].message\n",
        "\n",
        "\n",
        "# Call the helper method\n",
        "\n",
        "# Primary content or prompt text\n",
        "text = \"oh say can you see\"\n",
        "\n",
        "# Using the prompt\n",
        "response = get_completion(text)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "PE_GhX7z7E5P",
        "outputId": "80bbead8-6511-4717-9b97-e9e1062fec8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By the dawn's early light\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = get_completion(\"To be or\")\n",
        "print(response2.content)"
      ],
      "metadata": {
        "id": "l_tGo1Sh4tZd",
        "outputId": "f100848b-0568-477f-e8e0-f4e98fe81b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be, that is the question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9a8GliTiWW"
      },
      "source": [
        "### Exercise 3: Fabrications\n",
        "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WGts3eQ_TiWW",
        "outputId": "e64bd08c-d4d6-4976-85dc-ee7aa254cdd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='### Lesson Plan: The Martian War of 2076\\n\\n**Grade Levels:** High School\\n\\n**Subject:** History\\n\\n**Objective:** \\n- Students will gain an understanding of the causes, events, and consequences of the Martian War of 2076.\\n- Students will analyze primary and secondary sources related to the war and develop critical thinking skills.\\n\\n**Materials Needed:**\\n- Textbooks or online resources on the Martian War of 2076\\n- Primary and secondary sources such as newspaper articles, speeches, and personal accounts\\n- Laptops or devices with internet access\\n- Notebooks and writing utensils\\n\\n**Duration:** 2 class periods\\n\\n---\\n\\n**Lesson 1: Causes and Events of the Martian War**\\n\\n1. *Introduction* (15 mins):\\n   - Provide an overview of the Martian War of 2076.\\n   - Discuss the tensions between Earth and Mars leading up to the war.\\n\\n2. *Timeline Activity* (20 mins):\\n   - Divide students into groups and provide them with a timeline of key events leading to the war.\\n   - Ask students to discuss and identify the main causes of the conflict.\\n\\n3. *Primary Source Analysis* (30 mins):\\n   - Distribute primary sources related to the Martian War, such as speeches or newspaper articles.\\n   - Instruct students to analyze the sources and identify different perspectives on the conflict.\\n\\n4. *Class Discussion* (15 mins):\\n   - Lead a discussion on the causes and events of the Martian War based on the timeline and primary sources.\\n   - Encourage students to share their thoughts and reflections.\\n\\n---\\n\\n**Lesson 2: Consequences and Impact of the Martian War**\\n\\n1. *Review* (15 mins):\\n   - Recap the causes and events of the Martian War from the previous lesson.\\n\\n2. *Research Activity* (30 mins):\\n   - Assign students to research the consequences and impact of the Martian War on Earth and Mars.\\n   - Students can use textbooks, online resources, or primary sources for their research.\\n\\n3. *Group Presentation* (30 mins):\\n   - Have students present their findings on the consequences of the war.\\n   - Encourage discussion and debate on the long-term effects of the conflict.\\n\\n4. *Reflection and Discussion* (15 mins):\\n   - Lead a reflection session on the lessons learned from studying the Martian War of 2076.\\n   - Discuss the importance of understanding past conflicts in shaping future decisions.\\n\\n---\\n\\n**Assessment:**\\n- Students will be assessed based on their participation in class discussions, group activities, and their ability to analyze primary sources.\\n- An optional assessment could be a written reflection on the lessons learned from studying the Martian War of 2076.\\n\\n**Extension Activity:**\\n- Students can create a multimedia presentation or a short film depicting the key events of the Martian War.\\n- Organize a debate where students take on the roles of different stakeholders in the war and discuss their perspectives.\\n\\nBy the end of this lesson plan, students should have a deeper understanding of the Martian War of 2076 and its significance in interplanetary history.', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Set the text for simple prompt or primary content\n",
        "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
        "## Run the completion\n",
        "text = f\"\"\"\n",
        "generate a lesson plan on the Martian War of 2076.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9sz0gZ4TiWW"
      },
      "source": [
        "### Exercise 4: Instruction Based\n",
        "Use the \"text\" variable to set the primary content\n",
        "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
        "\n",
        "Here we ask the model to summarize the text for a second-grade student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxirf8asTiWX"
      },
      "outputs": [],
      "source": [
        "# Test Example\n",
        "# https://platform.openai.com/playground/p/default-summarize\n",
        "\n",
        "## Example text\n",
        "text = f\"\"\"\n",
        "Jupiter is the fifth planet from the Sun and the \\\n",
        "largest in the Solar System. It is a gas giant with \\\n",
        "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
        "times that of all the other planets in the Solar System combined. \\\n",
        "Jupiter is one of the brightest objects visible to the naked eye \\\n",
        "in the night sky, and has been known to ancient civilizations since \\\n",
        "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
        "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
        "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
        "natural object in the night sky after the Moon and Venus.\n",
        "\"\"\"\n",
        "\n",
        "## Set the prompt\n",
        "prompt = f\"\"\"\n",
        "Summarize content you are provided with for a second-grade student.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "## Run the prompt\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jksuygOBTiWX"
      },
      "source": [
        "### Exercise 5: Complex Prompt\n",
        "Try a request that has system, user and assistant messages\n",
        "System sets assistant context\n",
        "User & Assistant messages provide multi-turn conversation context\n",
        "\n",
        "Note how the assistant personality is set to \"sarcastic\" in the system context.\n",
        "Try using a different personality context. Or try a different series of input/output messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbdiW6poTiWX"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=deployment,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRGHRDfDTiWX"
      },
      "source": [
        "### Exercise: Explore Your Intuition\n",
        "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}